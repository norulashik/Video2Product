# -*- coding: utf-8 -*-
"""Video2product.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LDebrc5KdWu9FrZA3tDjORTXdKCX9X4k
"""

!pip install ultralytics opencv-python pillow transformers beautifulsoup4 requests

from google.colab import files
import torch, os, re, csv, requests, time, cv2
from PIL import Image
from ultralytics import YOLO
from bs4 import BeautifulSoup
from collections import defaultdict
import matplotlib.pyplot as plt
from IPython.display import display, clear_output

# ========== SETTINGS ==========
PIXEL_PER_CM = 17.74  # You can adjust based on your calibration object
FRAME_INTERVAL_SECONDS = 2
MAX_OBJECTS_PER_CLASS = 10
BUY_LINKS_PER_OBJECT = 10

# ========== STEP 1: Upload Video ==========
uploaded = files.upload()
video_path = list(uploaded.keys())[0]

# ========== STEP 2: Load YOLOv8 Model ==========
model = YOLO("yolov8n.pt")  # or yolov8s.pt for better accuracy

# ========== STEP 3: Process Video, Measure and Display ==========
print("\n🎯 Extracting and measuring objects from video...")

cap = cv2.VideoCapture(video_path)
fps = int(cap.get(cv2.CAP_PROP_FPS))
frame_interval = int(fps * FRAME_INTERVAL_SECONDS)
frame_count = 0
object_images = defaultdict(list)
detected_objects_summary = defaultdict(int)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    if frame_count % frame_interval == 0:
        results = model(frame)[0]
        annotated = frame.copy()

        for box in results.boxes:
            cls = int(box.cls)
            label = model.names[cls]
            x1, y1, x2, y2 = map(int, box.xyxy[0])

            w_px, h_px = x2 - x1, y2 - y1
            width_cm = w_px / PIXEL_PER_CM
            height_cm = h_px / PIXEL_PER_CM
            area_cm2 = (w_px * h_px) / (PIXEL_PER_CM**2)

            cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)
            label_text = f"{label}: {width_cm:.1f}x{height_cm:.1f}cm ({area_cm2:.1f}cm²)"
            cv2.putText(annotated, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 2)

            if len(object_images[label]) < MAX_OBJECTS_PER_CLASS:
                crop = frame[y1:y2, x1:x2]
                object_images[label].append(Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)))

            detected_objects_summary[label] += 1

        # Display annotated frame
        clear_output(wait=True)
        plt.figure(figsize=(10, 6))
        plt.axis('off')
        plt.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))
        plt.title(f"Frame {frame_count}")
        plt.show()

    frame_count += 1

cap.release()
print(f"✅ Objects detected: {list(object_images.keys())}")

# ========== STEP 4: Web Search Function ==========
def duckduckgo_search(query, max_results=20):
    headers = {'User-Agent': 'Mozilla/5.0'}
    params = {'q': query}
    results = []
    try:
        res = requests.post("https://html.duckduckgo.com/html/", data=params, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        for a in soup.select('a.result__a'):
            title = a.text.strip()
            href = a['href']
            if href.startswith('/l/?kh='):
                match = re.search(r'u=(.*?)&', href)
                if match:
                    href = requests.utils.unquote(match.group(1))
            if not href.endswith(('.jpg', '.jpeg', '.png', '.gif')):
                results.append({'title': title, 'link': href})
            if len(results) >= max_results:
                break
    except Exception as e:
        print(f"❌ Search error for '{query}': {e}")
    return results

def is_valid_link(link):
    whitelist = [
        "amazon.", "flipkart.", "ebay.", "aliexpress.", "walmart.",
        "snapdeal.", "myntra.", "shopify.", "bestbuy.", "target.com", "ajio."
    ]
    return any(domain in link.lower() for domain in whitelist)

# ========== STEP 5: Scrape Product Info ==========
def extract_product_data(link):
    headers = {'User-Agent': 'Mozilla/5.0'}
    price, rating, reviews, description = "Not Found", "N/A", "N/A", ""
    try:
        response = requests.get(link, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text()

        price_match = re.search(r'(\$|₹|Rs\.?)\s?\d{1,3}(?:[,\d]{3})*(?:\.\d{1,2})?', text)
        if price_match:
            price = price_match.group(0)

        rating_match = re.search(r'(\d\.\d)\s?out of\s?5', text)
        if rating_match:
            rating = rating_match.group(1) + "/5"

        review_match = re.search(r'([\d,]+)\s+(?:reviews|ratings)', text)
        if review_match:
            reviews = review_match.group(1)

        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc and meta_desc.get("content"):
            description = meta_desc["content"].strip()
        else:
            first_p = soup.find("p")
            if first_p:
                description = first_p.get_text().strip()[:300]

    except Exception as e:
        print(f"⚠️ Scrape error for {link}: {e}")
    return price, rating, reviews, description

# ========== STEP 6: Search & Scrape Products ==========
data_to_save = []

for obj_name in object_images.keys():
    print(f"\n🔍 Searching for: {obj_name}")
    search_results = duckduckgo_search(f"buy {obj_name} online", max_results=30)
    count = 0
    for result in search_results:
        if is_valid_link(result["link"]):
            print(f"\n🛒 {obj_name} → {result['title']}")
            print(f"🔗 {result['link']}")
            price, rating, reviews, desc = extract_product_data(result['link'])
            print(f"💰 Price: {price} | ⭐ Rating: {rating} | 🗣️ Reviews: {reviews}")
            data_to_save.append([obj_name, result['title'], result['link'], price, rating, reviews, desc])
            count += 1
        if count >= BUY_LINKS_PER_OBJECT:
            break

# ========== STEP 7: Save to CSV ==========
csv_filename = "video_objects_scraped.csv"
with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["Object Type", "Title", "Website Link", "Price", "Rating", "Reviews Count", "Description"])
    writer.writerows(data_to_save)

print(f"\n✅ Saved all product details to '{csv_filename}'")
files.download(csv_filename)